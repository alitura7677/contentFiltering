The Remez algorithm or Remez exchange algorithm , published by Evgeny Yakovlevich Remez in 1934 , is an iterative algorithm used to find simple approximations to functions , specifically , approximations by functions in a Chebyshev space that are the best in the uniform norm ' ' L ' '   sense . A typical example of a Chebyshev space is the subspace of Chebyshev polynomials of order ' ' n ' ' in the space of real continuous functions on an interval , ' ' C ' ' ' ' a ' ' , ' ' b ' ' . The polynomial of best approximation within a given subspace is defined to be the one that minimizes the maximum absolute difference between the polynomial and the function . In this case , the form of the solution is precised by the equioscillation theorem . # Procedure # The Remez algorithm starts with the function ' ' f ' ' to be approximated and a set ' ' X ' ' of  n + 2  sample points  x1 , x2 , ... , xn+2 @ @ @ @ @ @ @ @ @ @ linearly mapped to the interval . The steps are : # Solve the linear system of equations :  b0 + b1 xi+ .. +bn xi n + ( -1 ) i E = f(xi)  ( where  i=1 , 2 , .. n+2  ) , : for the unknowns  b0 , b1 ... bn  and ' ' E ' ' . # Use the  bi  as coefficients to form a polynomial  Pn  . # Find the set ' ' M ' ' of points of local maximum error  Pn(x) - f(x)  . # If the errors at every  m in M  are of equal magnitude and alternate in sign , then  Pn  is the minimax approximation polynomial . If not , replace ' ' X ' ' with ' ' M ' ' and repeat the steps above . The result is called the polynomial of best approximation , the Chebyshev approximation , or the minimax approximation . A review of technicalities in implementing the Remez algorithm is given by W. Fraser . # On the @ @ @ @ @ @ @ @ @ @ choice for the initial approximation because of their role in the theory of polynomial interpolation . For the initialization of the optimization problem for function ' ' f ' ' by the Lagrange interpolant ' ' L ' '  n  ( ' ' f ' ' ) , it can be shown that this initial approximation is bounded by :  lVert f - Ln(f)rVertinfty le ( 1 + lVert LnrVertinfty ) infp in Pn lVert f - prVert  with the norm or Lebesgue constant of the Lagrange interpolation operator ' ' L ' '  ' ' n ' '  of the nodes ( ' ' t ' '  1  , ... , ' ' t ' '  ' ' n ' ' + 1  ) being :  lVert LnrVertinfty = overlineLambdan(T) = max-1 le x le 1 lambdan ( T ; x ) ,  ' ' T ' ' being the zeros of the Chebyshev polynomials , and the Lebesgue functions being :  lambdan ( T ; x ) = sumj = 1n + 1 left lj(x) right @ @ @ @ @ @ @ @ @ @ 1 frac ( x - ti ) ( tj - ti ) .  Theodore A. Kilgore , Carl de Boor , and Allan Pinkus proved that there exists a unique ' ' t ' '  ' ' i ' '  for each ' ' L ' '  ' ' n ' '  , although not known explicitly for ( ordinary ) polynomials . Similarly ,  underlineLambdan(T) = min-1 le x le 1 lambdan ( T ; x )  , and the optimality of a choice of nodes can be expressed as  overlineLambdan - underlineLambdan ge 0 .  For Chebyshev nodes , which provides a suboptimal , but analytically explicit choice , the asymptotic behavior is known as :  overlineLambdan(T) = frac2pi log ( n + 1 ) + frac2pileft ( gamma + logfrac8piright ) + alphan + 1  ( ' ' ' ' being the Euler-Mascheroni constant ) with :  0 **38;224707; for  n ge 1 ,  and upper bound :  overlineLambdan(T) le frac2pi log ( n + 1 ) + 1  Lev Brutman obtained @ @ @ @ @ @ @ @ @ @  hatT  being the zeros of the expanded Chebyshev polynomials : :  overlineLambdan(hatT) - underlineLambdan(hatT) **153;224747; Rdiger Gnttner obtained from a sharper estimate for  n ge 40  :  overlineLambdan(hatT) - underlineLambdan(hatT) **16;224902; # Detailed Discussion # Here we provide more information on the steps outlined above . In this section we let the index ' ' i ' ' run from 0 to ' ' n ' ' +1 . Step 1 : Given  x0 , x1 , .. xn+1  , solve the linear system of ' ' n ' ' +2 equations :  b0 + b1 xi+ .. +bn xi n + ( -1 ) i E = f(xi)  ( where  i=0 , 1 , .. n+1  ) , : for the unknowns  b0 , b1 , ... bn  and ' ' E ' ' . It should be clear that  ( -1 ) i E  in this equation makes sense only if the nodes  x0 , ... , xn+1  are ' ' ordered ' ' , either strictly increasing or strictly decreasing @ @ @ @ @ @ @ @ @ @ ( As is well known , not every linear system has a solution . ) Also , the solution can be obtained with only  O(n2)  arithmetic operations while a standard solver from the library would take  O(n3)  operations . Here is the simple proof : Compute the standard ' ' n ' ' -th degree interpolant  p1(x)  to  f(x)  at the first ' ' n ' ' +1 nodes and also the standard ' ' n ' ' -th degree interpolant  p2(x)  to the ordinates  ( -1 ) i  :  p1(xi) = f(xi) , p2(xi) = ( -1 ) i , i = 0 , ... , n .  To this end use each time Newton 's interpolation formula with the divided differences of order  0 , ... , n  and  O(n2)  arithmetic operations . The polynomial  p2(x)  has its ' ' i ' ' -th zero between  xi-1  and  xi , i=1 , ... , n  , and thus no further zeroes between  xn  @ @ @ @ @ @ @ @ @ @ p2(xn+1)  have the same sign  ( -1 ) n  . The linear combination  p(x) : = p1 ( x ) - p2(x) ! cdot ! E  is also a polynomial of degree ' ' n ' ' and :  p(xi) = p1(xi) - p2(xi) ! cdot ! E = f(xi) - ( -1 ) i E , i =0 , ldots , n .  This is the same as the equation above for  i = 0 , .. , n  and for any choice of ' ' E ' ' . The same equation for ' ' i ' ' = ' ' n ' ' +1 is :  p(xn+1) = p1(xn+1) - p2(xn+1) ! cdot ! E = f(xn+1) - ( -1 ) n+1 E  and needs special reasoning : solved for the variable ' ' E ' ' , it is the ' ' definition ' ' of ' ' E ' ' : :  E : = fracp1(xn+1) - f(xn+1)p2(xn+1) + ( -1 ) n .  As mentioned above , the two terms in the @ @ @ @ @ @ @ @ @ @ and thus  p(x) equiv b0 + b1x + ldots + bnxn  are always well-defined . The error at the given ' ' n ' ' +2 ordered nodes is positive and negative in turn because :  p(xi) - f(xi) = -(-1)i E , i = 0 , .. , n ! + ! 1 .  The Theorem of ' ' de La Valle Poussin ' ' states that under this condition no polynomial of degree ' ' n ' ' exists with error less than ' ' E ' ' . Indeed , if such a polynomial existed , call it  tilde p(x)  , then the difference  p(x)-tilde p(x) = ( p(x) - f(x) - ( tilde p(x) - f(x)  would still be positive/negative at the ' ' n ' ' +2 nodes  xi  and therefore have at least ' ' n ' ' +1 zeros which is impossible for a polynomial of degree ' ' n ' ' . Thus , this ' ' E ' ' is a lower bound for the minimum error which can be achieved with @ @ @ @ @ @ @ @ @ @ 2 changes the notation from  b0 + b1x + .. + bnxn  to  p(x)  . Step 3 improves upon the input nodes  x0 , ... , xn+1  and their errors  pm E  as follows . In each P-region , the current node  xi  is replaced with the local maximizer  barxi  and in each N-region  xi  is replaced with the local minimizer . ( Expect  barx0  at ' ' A ' ' , the  bar xi  near  xi  , and  barxn+1  at ' ' B ' ' . ) No high precision is required here , the standard ' ' line search ' ' with a couple of ' ' quadratic fits ' ' should suffice . ( See ) Let  zi : = p(barxi) - f(barxi)  . Each amplitude  zi  is greater than or equal to ' ' E ' ' . The Theorem of ' ' de La Valle Poussin ' ' and its proof also apply to  z0 , .. , zn+1 @ @ @ @ @ @ @ @ @ @ lower bound for the best error possible with polynomials of degree ' ' n ' ' . Moreover ,  maxzi  comes in handy as an obvious upper bound for that best possible error . Step 4 : With  min , zi  and  max , zi  as lower and upper bound for the best possible approximation error , one has a reliable stopping criterion : repeat the steps until  maxzi - minzi  is sufficiently small or no longer decreases . These bounds indicate the progress . # Variants # Sometimes more than one sample point is replaced at the same time with the locations of nearby maximum absolute differences . Sometimes relative error is used to measure the difference between the approximation and the function , especially if the approximation will be used to compute the function on a computer which uses floating point arithmetic . 